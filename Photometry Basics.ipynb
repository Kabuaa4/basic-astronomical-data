{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Photometry\n",
    "\n",
    "Photometry refers to the measurement of the total flux being received from the object.  However, themeasurement of the exact flux is highly complicated as a process.  Even more difficult is the processof quantifying the level of uncertainty in your final measurement.  This is beyond the scope of thetutorial, our job here is to tell you how very easily, you can do quick look photometry using IRAF’simexamine task.  But first things first, let us revise the concepts of photometry.\n",
    "\n",
    "Brightness  of  an  object  i.e.   the  amount  of  flux  received  from  the  source  is  measured  in  a  systemknown asmagnitudes.  The reasons for using this are partly historical and the reader is encouragedto refer the excellent book calledAstronomical PhotometrybyHenden & Kaitchuck.  The system isdefined as follows - if the ratio of flux from two stars is 100, then the difference in their magnitudesis  5.   This,  for  the  mathematically  oriented,  should  quickly  tell  you  that  the  magnitude  scale  is  alogarithmic scale.\n",
    "\n",
    "$$\n",
    "m_1 - m_2 = -2.5 \\log(f1 / f2)\n",
    "$$\n",
    "\n",
    "But if you have seen any table stating the brightnesses of these objects, you will see values like -3.0and 1.5 etc.  But the above equation tells you about the measure of difference in magnitudes.  So, howdoes one assign a single unique value to an object?  There are many schemes to do this but the mostcommon one is called the Vega system.  In this sytem,  the magnitude of the star Vega is assigned0.  Then, to assign a brightness to any object, its flux is compared with that of Vega and the above equation can be used to assign it magnitude.\n",
    "\n",
    "NOTE: Carefully study the above equation.  The−sign tells you that the magnitude scale has aninverse  sense.  A magnitude 5 star is  fainter than a magnitude 1 star.  This is very unintuitive atfirst  but  with  time,  you  will  praise  the  fact  that  this  is  inverse.   (Thought  Exercise:  Think  about advantages of having an inverse system like this.)\n",
    "\n",
    "Next comes the question of flux measurement.  Take a look at any image of say, a star and we know that it occupies a finite space.  We just mark a suitable region, sum up all the fluxes in each pixel and done - we have a flux measurement.  Unfortunately, this flux measurement is not going to be correct. This is because it includes contributions from several other sources - the moon in the sky, distant butunseen galaxies or stars, sky emission etc.  This needs to be subtracted.\n",
    "\n",
    "This means that we need a method by which we can get the per pixel contribution from such sourcesand subtract them from our measured flux.  To do this, we will choose an annular region sufficientlyaway from the source of interest, unoccupied by any other source, measure the average value of thepixels  and  subtract  it  from  what  we  consider  to  be  a  source  measurement.   This  process  is  calledaperture photometry and is illustrated in Figure below.  The pink region is used to get sky contribution.The red region is assumed to contain flux from the source.\n",
    "\n",
    "![Aperture Photometry Explained](aperture.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photometry with imexamine\n",
    "\n",
    "It is possible to do quick look photometry with imexam introduced in the image examination tool earlier. We do not explain this here but the concept is simple. Just start the imexam task and use 'a' command in order to do the aperture photometry. Remember to consider the following points.\n",
    "\n",
    "- The flux has to be corrected for extinction - a rather involved process but routinely done at any observatory.\n",
    "- In many cases it is desirable to correct the flux for foreground extinction as well\n",
    "- Another process is to standardize your filters - this is what determines the 'zero point'.\n",
    "- The photometry being done by imexam and one explained above is called 'aperture photometry'. The larger the aperture the better but it is also impractical. Hence we normally choose some decent aperture size for measurement. But the growth curve tells you an estimate of what percentage of flux is lost because of having chosen a finite size aperture. This needs to be corrected and is called aperture correction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photometry on Steroids\n",
    "\n",
    "In the circumstances where you an image containing hundreds of stars (even thousands) and when there are thousands of such images being taken every day or sometimes even every hour, we cannot of course rely on imexam like methods to measure the magnitude of the star. What is needed in this case is a method which can\n",
    "\n",
    "- automatically detect all the objects in the image\n",
    "- decide which of these objects are stars and which are extended objects\n",
    "- perform measurements\n",
    "- produce catalogs\n",
    "\n",
    "This is what we will take up next. We will assume that 'final.fits' which we produced in the basic reduction tutorial is indeed a final image and use it. We don't care about exactness of answers at this point but we simply care about getting a feel for what kind of methods are available for such an undertaking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits\n",
    "import os\n",
    "hdulist = fits.open('IRAF_Exercises_DemoData/final.fits')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Filename: IRAF_Exercises_DemoData/final.fits\n",
      "No.    Name      Ver    Type      Cards   Dimensions   Format\n",
      "  0  PRIMARY       1 PrimaryHDU      13   (1340, 1300)   float64   \n",
      "  1  MASK          1 ImageHDU         8   (1340, 1300)   uint8   \n",
      "  2  UNCERT        1 ImageHDU         9   (1340, 1300)   float64   \n"
     ]
    }
   ],
   "source": [
    "hdulist.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when used the `CCDData.write()` method, it produce three extensions - the first extension is the actual image, the second is a mask which essentially is a log of which pixels in the image are good and which are bad. The third extension is an estimate of the error bars on each pixel. We will not concern ourselves with any of the newly added extensions for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " id     xcentroid      ...        flux                mag        \n",
      "--- ------------------ ... ------------------ -------------------\n",
      "  1  94.02734652655029 ... 1.0922165662457497 -0.0957718985782663\n",
      "  2  94.70867499909988 ... 1.3134980941999033 -0.2960736174739207\n",
      "  3  719.0081416820913 ...   3.93584720158764 -1.4875955745027902\n",
      "  4  779.9527474420936 ... 15.000215036855176 -2.9402437124142375\n",
      "  5 1121.7319011126772 ...  7.479612366583009  -2.184697727476579\n",
      "  6  871.3225549371747 ... 4.6985779497867055 -1.6799160905789388\n",
      "  7   852.269478280032 ... 3.1859021679443873  -1.258081088601377\n",
      "  8  1161.286508218904 ... 1.2652368440395638 -0.2554295755019025\n",
      "  9   712.913797228197 ... 106.83597364490946  -5.071793780721991\n",
      " 10 300.46438739173374 ... 106.74849958213953  -5.070904448638981\n",
      "...                ... ...                ...                 ...\n",
      "130  1169.576856822971 ...  49.55658758469512  -4.237753484163215\n",
      "131  1174.808572211921 ...   42.0326864364819  -4.058967869925291\n",
      "132 1207.8546196335676 ... 30.307446568990027  -3.703873370471109\n",
      "133 1215.4784602204845 ...  62.68050201908754  -4.492831165377088\n",
      "134 1222.8734958003984 ...   41.6246628344991  -4.048376821708789\n",
      "135 1238.7936283791441 ... 37.268156113476955 -3.9283447651183057\n",
      "136  1265.553691633658 ... 24.639313888480636   -3.47907152551308\n",
      "137 1290.0557519737745 ...  59.75125204831978  -4.440867525179279\n",
      "138 1293.9788664212858 ...  64.91708131763764  -4.530897464109338\n",
      "139 1312.6539700906171 ...   40.9948211435489  -4.031822489918835\n",
      "140 1336.5967265053403 ... 31.816307528360763 -3.7566244394463686\n",
      "Length = 140 rows\n"
     ]
    }
   ],
   "source": [
    "from photutils import DAOStarFinder\n",
    "import numpy as np\n",
    "from astropy.stats import mad_std\n",
    "bkg_sigma = mad_std(hdulist[0].data)\n",
    "background = np.median(hdulist[0].data)\n",
    "daofind = DAOStarFinder(fwhm=4., threshold=background + 3.*bkg_sigma)\n",
    "sources = daofind(hdulist[0].data)\n",
    "\n",
    "print(sources)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " id      xcenter            ycenter          aperture_sum   \n",
      "           pix                pix                           \n",
      "--- ------------------ ------------------ ------------------\n",
      "  1  94.02734652655029  4.436052963600958  1891.179039264925\n",
      "  2  94.70867499909988 3.9091982853019367 1855.8318708564639\n",
      "  3  719.0081416820913 26.977825124239097   4299.52158271193\n",
      "  4  779.9527474420936 35.971866796294734 18218.083478500666\n",
      "  5 1121.7319011126772 55.797124736626564 7358.6015423772915\n",
      "  6  871.3225549371747  56.57000726916798  4896.959840769957\n",
      "  7   852.269478280032 108.39720262603237 3743.2229667681836\n",
      "  8  1161.286508218904  142.3817469570009 1363.3312000736187\n",
      "  9   712.913797228197 151.79969671556879 105403.76161880133\n",
      " 10 300.46438739173374 174.74616696783738  112573.6609061783\n",
      "...                ...                ...                ...\n",
      "131  1174.808572211921  1298.986352855166  3503.034896082477\n",
      "132 1207.8546196335676  1298.954717032067   889.627346896063\n",
      "133 1215.4784602204845  1298.989923536311  3418.694968542478\n",
      "134 1222.8734958003984 1298.9796584257601 3783.5682216085092\n",
      "135 1238.7936283791441 1298.9618694930782 3281.1130104520066\n",
      "136  1265.553691633658 1298.9702551191497 1540.0658371800348\n",
      "137 1290.0557519737745 1298.9865371556775 3201.9858023690717\n",
      "138 1293.9788664212858 1298.9882758353003  2550.156767151252\n",
      "139 1312.6539700906171 1298.9825192866617 2199.6193756354023\n",
      "140 1336.5967265053403  1298.968750213407  2758.165642502393\n",
      "Length = 140 rows\n"
     ]
    }
   ],
   "source": [
    "from photutils import aperture_photometry, CircularAperture\n",
    "positions = np.transpose((sources['xcentroid'], sources['ycentroid']))\n",
    "apertures = CircularAperture(positions, r=4.)\n",
    "phot_table = aperture_photometry(hdulist[0].data, apertures)\n",
    "\n",
    "print(phot_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz**\n",
    "\n",
    "- What is a very subtle assumption in aperure photometry implicit within its very procedure?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crowded Fields\n",
    "\n",
    "Think on the following:\n",
    "\n",
    "- Can you really apply aperture photometry for crowded fields? (What is a crowded field anyway? When do you say it is crowded?)\n",
    "- What are the problems faced in aperture photometry?\n",
    "\n",
    "What is the solution when one wants to do a crowded field photometry?\n",
    "\n",
    "__Answer__: We do PSF photometry.\n",
    "\n",
    "How it works?\n",
    "\n",
    "- We choose a model that describes our PSF\n",
    "- For ground based telescopes, a Gaussian or a Moffat works fine\n",
    "- For space based telescopes, one needs something much more complex - generally available from the engineers who worked on the telescope (eg. TinyTim for Hubble Space Telescope, library of PSFs for Spitzer etc.)\n",
    "- We perform detection of objects as usual.\n",
    "- And we then fit the PSF model with following free parameters - center, normalization\n",
    "- The normalization gives us the flux belonging to a star."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
